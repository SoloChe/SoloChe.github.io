<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://soloche.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://soloche.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-26T08:24:14+00:00</updated><id>https://soloche.github.io/feed.xml</id><title type="html">Yiming Che (车一鸣)</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Notes of Score Diffusion Models (updating)</title><link href="https://soloche.github.io/blog/2024/SDM-Notes/" rel="alternate" type="text/html" title="Notes of Score Diffusion Models (updating)"/><published>2024-09-24T00:00:00+00:00</published><updated>2024-09-24T00:00:00+00:00</updated><id>https://soloche.github.io/blog/2024/SDM-Notes</id><content type="html" xml:base="https://soloche.github.io/blog/2024/SDM-Notes/"><![CDATA[ <h1 id="background">Background</h1> <p>In score-based diffusion models <d-cite key="songscore"></d-cite>, the authors proposed a unified framework that connects the score-based model NCSN and DDPM through the perspective of Stochastic Differential Equations (SDE). They interpret the forward process (adding noise) and the backward process (denoising sampling) as SDE and reverse SDE, respectively.</p> <h2 id="data-perturbation-with-itô-sde">Data Perturbation with Itô SDE</h2> <p>The diffusion process ${\x_t}_{t=0}^T$ from the original input $\x_0$ to Gaussian noise $\x_T$ with continuous time variable $t\in\sbr{0,T}$ can be modeled by the Itô SDE</p> \[\begin{equation} d\x = \f(\x, t) dt + \bs{G}(\x, t)d\w_t \end{equation}\] <ul> <li>$\w_t$ is the standard Wiener process.</li> <li>$\f(\x,t): \mathbb{R}^d \rightarrow \mathbb{R}^d$ are the drift coefficients and $d$ is the dimension of $\x$. It is always affine, resulting in $f(\x, t) = f(t)\x$ where $f(\cdot):\mathbb{R} \rightarrow \mathbb{R}$.</li> <li>$\bs{G}(\x,t): \mathbb{R}^d \rightarrow \mathbb{R}^{d\times d}$ are the diffusion coefficients at time $t$.</li> </ul> <p>We consider the case $\bs{G}(\x,t) = g(t)\I$ which is independent of $\x$. Then, Eq. (1) can be rewritten as \(\begin{equation} d\x = f(t)\x dt + g(t)d\w_t \end{equation}\)</p> <p>The perturbation kernel of this SDE have the form</p> \[\begin{equation} p(\x_t \mid \x_0) = \N(\x_t \mid s_t\x_0, s_t^2\sigma_t^2\I) \end{equation}\] <p>where</p> \[\begin{equation} s_t = \exp\nbr{\int_0^t f(\xi)d\xi} \quad \text{and}\quad \sigma_t^2 = \int_0^t\nbr{\frac{g(\xi)}{s(\xi)}}^2d\xi \end{equation}\] <p>Here, I’ll use $s_t$ and $s(t)$, and $\sigma_t$ and $\sigma(t)$ interchangeably for simplicity. The corresponding marginal distribution is obtained by</p> \[\begin{align} p(\x_t) &amp;= \int p(\x_t \mid \x_0)p_{data}(\x_0)d\x_0\\ &amp;= s_t^{-d}\int p_{data}(\x_0)\N(\x_ts_t^{-1} \mid \x_0, \sigma_t^2\I)d\x_0\\ &amp;= s_t^{-d}\int p_{data}(\x_0)\N(\x_ts_t^{-1}-\x_0 \mid 0, \sigma_t^2\I)d\x_0\\ &amp;= s_t^{-d} \sbr{p_{data}(\x_0) * \N\nbr{0, \sigma_t^2\I}}(\x_ts_t^{-1})\\ &amp;= s_t^{-d} p(\x_ts_t^{-1};\sigma_t) \label{eq:marginal} \end{align}\] <p>The <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank equation</a> describes the evolution of the marginal distribution $p_t(\x)$, interchangeable with $p(\x_t)$, over time under the effect of drift forces and random (or noise) forces. It can be written as</p> \[\begin{align} \frac{\partial p_t(\x)}{\partial t} &amp;= -\nabla_{\x}\cdot\sbr{\f(\x,t) p_t(\x)} + \frac{1}{2}\nabla_{\x}\nabla_{\x}\cdot\sbr{\bs{G}(\x,t)\bs{G}^T(\x,t)p(\x_t)}\\ &amp;= -\sum_{i=1}^{d}\frac{\partial}{\partial x_i}\sbr{f_i(\x,t)p_t(\x)} + \frac{1}{2}\sum_{i=1}^{d}\sum_{j=1}^{d}\frac{\partial^2}{\partial \x_i \partial \x_j}\sbr{\sum_{k=1}^{d} G_{ik}(\x, t)G_{jk}(\x, t) p_t(\x)}\\ &amp;= -\sum_{i=1}^{d}\frac{\partial}{\partial x_i}\sbr{f_i(\x,t)p_t(\x) -\frac{1}{2}\sbr{\nabla_{\x}\sbr{\bs{G}(\x,t)\bs{G}^T(\x,t)} + \bs{G}(\x,t)\bs{G}^T(\x,t)\nabla_{\x}\log p_t(\x)}p_t(\x)}\\ &amp;= -\sum_{i=1}^{d}\frac{\partial}{\partial x_i}\sbr{\tilde{f}_i(\x,t)p_t(\x)} \label{eq:fokker-planck} \end{align}\] <p>where</p> \[\tilde{f}_i(\x,t) = f_i(\x,t) - \frac{1}{2}\sbr{\nabla_{\x}\sbr{\bs{G}(\x,t)\bs{G}^T(\x,t)} + \bs{G}(\x,t)\bs{G}^T(\x,t)\nabla_{\x}\log p_t(\x)}.\] <p>If we consider the case $\bs{G}(\x,t) = g(t)\I$, then Eq. $\ref{eq:fokker-planck}$ can be rewritten as</p> \[\begin{align} \frac{\partial p_t(\x)}{\partial t} &amp;= -\nabla_{\x}\sbr{f(\x,t)p_t(\x)} + \frac{1}{2}g^2(t)\nabla_{\x}\nabla_{\x}\sbr{p(\x_t)}\\ &amp;= -\sum_{i=1}^{d}\frac{\partial}{\partial x_i}\sbr{\sbr{f_i(\x,t)-\frac{1}{2}g^2(t)\nabla_{\x}\log p_t(\x)}p_t(\x)} \label{eq:fokker-planck-special} \end{align}\] <h2 id="probability-flow-ode">Probability Flow ODE</h2> <p>According to the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank equation</a>, there exists an ODE which shares the same marginal distribution $p(\x)$ as the SDE. From Eq. $\ref{eq:fokker-planck-special}$, the corresponding SDE is reduced to ODE given by</p> \[\begin{align} d\x &amp;= \sbr{f(\x, t) - \frac{1}{2}g^2(t)\nabla_{\x}\log p(\x)}dt + 0d\w_t \\ &amp;= \sbr{f(t)\x - \frac{1}{2}g^2(t)\nabla_{\x}\log p(\x)}dt \label{eq:prob-flow-ode} \end{align}\] <p>The ODE is named as the probability flow ODE (PF ODE).</p> <p>If we further build the ODE according to $s_t$ and $\sigma_t$, we have</p> \[\begin{equation} f(t) = \dot{s}_ts_t^{-1} \quad \text{and} \quad g_t = s_t\sqrt{2\dot{\sigma}_t\sigma_t}. \end{equation}\] <p>The derivation can be found in EDM paper <d-cite key="karras2022elucidating"></d-cite> (Eq. 28 and 34). Then, Eq. $\ref{eq:prob-flow-ode}$ can be rewritten as</p> \[\begin{align} d\x &amp;= \sbr{\dot{s}_ts_t^{-1}\x - s_t^2\dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x)}dt\\ &amp;= \sbr{\dot{s}_ts_t^{-1}\x - s_t^2\dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x s_t^{-1};\sigma_t)}dt\\ &amp;= \sbr{ -\dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x_t;\sigma_t)}dt \quad \text{where} \quad s_t = 1 \end{align}\] <p>where the last step is due to the marginal $p(\x_t) = s^{-1}_t p(\x_t s_t^{-1};\sigma_t)$ where</p> \[p(\x_ts_t^{-1}; \sigma_t) = \sbr{p_{data}(\x_0) * \N\nbr{0, \sigma_t^2\I}}(\x_ts_t^{-1})\] <p>as in Eq. $\ref{eq:marginal}$.</p> <h2 id="connection-between-pf-ode-and-sde">Connection Between PF ODE and SDE</h2> <p>According to the Eq. (102) in <d-cite key="karras2022elucidating"></d-cite>, the authors derived a family of SDE with the marginal $p(\x_t) = p(\x_t;\sigma_t)$ for any choice of $g(t)$. The SDE is given by</p> \[\begin{align} d\x &amp;= \nbr{\frac{1}{2}g^2(t) - \dot{\sigma}_t\sigma_t}\nabla_{\x}\log p(\x;\sigma_t)dt + g(t)d\w_t\\ &amp;= \hat{\f}(\x, t)dt + g(t)d\w_t \label{eq:general-sde} \end{align}\] <p>The PF ODE is a special case of the SDE when $g(t) = 0$. The derivation is given in the EDM paper <d-cite key="karras2022elucidating"></d-cite> (Appendix B.5). It is a little bit long but not difficult to follow. Furthermore, the authors parameterized $g(t) = \sqrt{2\beta(t)}\sigma_t$ and Eq. $\ref{eq:general-sde}$ becomes</p> \[\begin{equation} d\x = - \dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x;\sigma_t)dt + \beta(t)\sigma_t^2\nabla_{\x}\log p(\x;\sigma_t)dt + \sqrt{2\beta(t)}\sigma_td\w_t \label{eq:general-sde-beta} \end{equation}\] <p>where $\beta(t)$ is a free function.</p> <h2 id="reverse-sde">Reverse SDE</h2> <p>The reverse diffusion process from $\x_T$ to $\x_0$ can also be modeled by Itô SDE according to <d-cite key="anderson1982reverse"></d-cite>:</p> \[\begin{align} d\x &amp;= \sbr{\hat{\f}(\x,t) - g^2(t)\nabla_{\x}\log p_t(\x)}dt + g(t)d\w_t \\ &amp;= - \dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x;\sigma_t)dt \underbrace{- \beta(t)\sigma_t^2\nabla_{\x}\log p(\x;\sigma_t)dt + \sqrt{2\beta(t)}\sigma_td\w_t}_{\text{Langevin dynamics: noise cancellation}} \end{align}\]]]></content><author><name>Yiming Che</name></author><category term="Bayes"/><category term="Deep-Learning"/><category term="Diffusion-Model"/><summary type="html"><![CDATA[This is a summary of unified framework of diffusion models by Stochastic Differential Equations (SDE) and the related topics.]]></summary></entry><entry><title type="html">Notes of Some Useful Knowledge, Tricks, and Tips (updating)</title><link href="https://soloche.github.io/blog/2024/all/" rel="alternate" type="text/html" title="Notes of Some Useful Knowledge, Tricks, and Tips (updating)"/><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://soloche.github.io/blog/2024/all</id><content type="html" xml:base="https://soloche.github.io/blog/2024/all/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#basics-of-gaussian-distribution">Basics of Gaussian Distribution</a></li> </ul> <h1 id="basics-of-gaussian-distribution">Basics of Gaussian Distribution</h1> <p>For a single variable $x$ that follows a Gaussian distribution, we have the probability density function (pdf) as follows:</p> \[\begin{equation} p(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \end{equation}\] <p>For a $D$-dimensional vector $\bm{x}$, the pdf is:</p> \[\begin{equation} p(\x|\bm{\mu}, \bm{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\bm{\Sigma}|^{1/2}} \exp\nbr{-\frac{1}{2}(\bm{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu})} \end{equation}\] <p>where $\bm{\mu}$ is a $D$-dimensional mean vector and $\bm{\Sigma}$ is a $D\times D$ covariance matrix.</p>]]></content><author><name></name></author><category term="Bayes"/><summary type="html"><![CDATA[miscellaneous notes]]></summary></entry><entry><title type="html">Notes of Diffusion Models</title><link href="https://soloche.github.io/blog/2023/DM-Notes/" rel="alternate" type="text/html" title="Notes of Diffusion Models"/><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://soloche.github.io/blog/2023/DM-Notes</id><content type="html" xml:base="https://soloche.github.io/blog/2023/DM-Notes/"><![CDATA[<p><strong>Table of Contents</strong></p> <ul> <li><a href="#revisit-of-denoising-diffusion-probabilistic-models-ddpm">Revisit of Denoising Diffusion Probabilistic Models (DDPM)</a> <ul> <li><a href="#ddpm-formulation">DDPM Formulation</a></li> <li><a href="#ddpm-forward-process-encoding">DDPM Forward Process (Encoding)</a></li> <li><a href="#ddpm-reverse-process-decoding">DDPM Reverse Process (Decoding)</a></li> <li><a href="#training-elbo">Training: ELBO</a> <ul> <li><a href="#parameterization-on-l_t">Parameterization on $L_t$</a></li> <li><a href="#l_t-and-l_0">$L_T$ and $L_0$</a></li> </ul> </li> <li><a href="#implementation">Implementation</a></li> </ul> </li> <li><a href="#connection-with-ddim">Connection with DDIM</a> <ul> <li><a href="#accelerated-inference">Accelerated Inference</a></li> </ul> </li> <li><a href="#connection-with-score-based-dm">Connection with Score-based DM</a> <ul> <li><a href="#score-and-score-matching">Score and Score Matching</a></li> <li><a href="#ddpm-to-score-matching-tweedies-formula">DDPM to Score Matching (Tweedie’s Formula)</a></li> <li><a href="#guidance-from-score">Guidance from Score</a> <ul> <li><a href="#classifier-guidance">Classifier Guidance</a></li> <li><a href="#classifier-free-guidance">Classifier-free Guidance</a></li> </ul> </li> </ul> </li> <li><a href="#noise-conditional-score-networks-ncsn">Noise Conditional Score Networks (NCSN)</a> <ul> <li><a href="#langevin-dynamics-sde">Langevin Dynamics (SDE)</a> <ul> <li><a href="#sampling">Sampling</a></li> </ul> </li> </ul> </li> </ul> <h1 id="revisit-of-denoising-diffusion-probabilistic-models-ddpm">Revisit of Denoising Diffusion Probabilistic Models (DDPM)</h1> <p>Some good reviews:</p> <ol> <li><a href="https://theaisummer.com/diffusion-models/?fbclid=IwAR1BIeNHqa3NtC8SL0sKXHATHklJYphNH-8IGNoO3xZhSKM_GYcvrrQgB0o">How diffusion models work: the math from scratch</a></li> <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li> <li><a href="https://yang-song.net/blog/2021/score/">Generative Modeling by Estimating Gradients of the Data Distribution</a></li> <li><a href="https://ar5iv.labs.arxiv.org/html/2208.11970">Understanding Diffusion Models: A Unified Perspective</a></li> <li><a href="https://arxiv.org/abs/2101.03288">How to Train Your Energy-Based Models</a></li> </ol> <h2 id="ddpm-formulation">DDPM Formulation</h2> <p>Given the data distribution $\x_0\sim q(\x_0)$ which is unknown, we want to learn an approximation $p_\theta (\x_0)$ that we can sample from. It is similar to variational autoencoder (VAE) or hierarchical VAE in the form, e.g., it also has encoding process (forward process) and decoding process (reverse process) and minimizes ELBO, but with multiple high dimensional latent variables.</p> <p>Diffusion models are latent variable models with the formulation (Markov chain)</p> \[p_\theta(\x_0) = \int p_\theta(\x_{0:T})d\x_{1:T} \space \text{where} \space p_\theta(\x_{0:T}) = p(\x_T)\prod_{t=1}^{T}p_\theta(\x_{t-1}\mid\x_t).\] <p>The latent variables are ${\x_1,…,\x_T}$ with the same dimensionality as the data $\x_0$ and their joint $p_\theta(\x_{0:T})$ is called the reverse process (decoding) starting at $p(\x_T)=\N(0,\I)$. The approximate posterior $q(\x_{1:T}\mid\x_0)$, called the forward process, is fixed to another Markov chain</p> \[q(\x_{1:T}\mid\x_0) = \prod_{t=1}^{T}q(\x_t\mid\x_{t-1}).\] <p>Compared to other latent variable models, e.g., VAE, it has no learnable parameters in the approximate posterior $q$ (encoding). In this process, the data $\x_0$ is transformed to $\x_T$ by gradually adding Gaussian noise. To recover the data distribution, the ELBO is maximized as</p> \[\begin{align*} \max_{\theta} \log p_\theta(\x_0) &amp;= \log \int \frac{p_\theta(\x_{0:T})q(\x_{1:T}\mid\x_0)}{q(\x_{1:T}\mid\x_0)}d\x_{1:T}\\ &amp;= \log \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{\frac{p_\theta(\x_{0:T})}{q(\x_{1:T}\mid\x_0)}} \\ &amp;\geq \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{\log \frac{p_\theta(\x_{0:T})}{q(\x_{1:T}\mid\x_0)}}\\ &amp;= -KL\sbr{q(\x_{1:T}\mid\x_0)\mid p_\theta(\x_{0:T})}. \end{align*}\] <h2 id="ddpm-forward-process-encoding">DDPM Forward Process (Encoding)</h2> <p>The original data $\x_0\sim q(\x_0)$ and the Markov chain assumes we add noise to the data $\x_0$ in each time step $t\in[1,T]$ with transition kernel $q(\x_t\mid\x_{t-1})$ which is usually handcrafted as</p> \[q(\x_t\mid\x_{t-1}) = \N\nbr{\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\I}\] <p>where $\beta_t\in (0,1)$ is a hyperparameter. A closed form of dependence according to the reparameterization trick:</p> \[\begin{align*} \x_t &amp;= \sqrt{1-\beta_t}\x_{t-1} + \sqrt{\beta_t}\bm{\epsilon}_{t-1} \quad \text{where} \quad \bm{\epsilon}_{t-1}\sim \N\nbr{\bm{0},\bm{I}}\\ &amp;= \sqrt{\alpha_t}\x_{t-1} + \sqrt{1-\alpha_t}\bm{\epsilon}_{t-1} \quad \text{where} \quad \alpha_t = 1-\beta_t \end{align*}\] <p>Furthermore, with the addition of two Gaussian random variable, we can trace back to the data $\x_0$</p> \[\begin{align} \x_t &amp;= \sqrt{\alpha_t\alpha_{t-1}}\x_{t-2} + \sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\bm{\epsilon}_{t-2} + \sqrt{1-\alpha_t}\bm{\epsilon}_{t-1} \nonumber\\ &amp;= \sqrt{\alpha_t\alpha_{t-1}}\x_{t-2} + \sqrt{1-\alpha_{t}\alpha_{t-1}} \bm{\epsilon} \quad \text{where} \quad \bm{\epsilon}\sim \N\nbr{\bm{0},\bm{I}}\nonumber\\ &amp; \qquad ...\nonumber\\ &amp;= \sqrt{\bar{\alpha}_t}\x_{0} + \sqrt{1-\bar{\alpha}_{t}} \bm{\epsilon} \end{align}\\\] <p>where $\bar{\alpha}_ t = \prod_{i=1}^t \alpha_i $. Usually, $\alpha_i$ will decrease along with $t$, and therefore $\bar{\alpha}_t \rightarrow 0$ when $t \rightarrow \infty$.</p> <h2 id="ddpm-reverse-process-decoding">DDPM Reverse Process (Decoding)</h2> <p>To generate a new sample or reverse from $\x_T\sim\N(0,\I)$, we need to know $q(\x_{t-1} \mid \x_t)$ which is unavailable in practice for decoding. However, we know it is also Gaussian according to the Bayes’ theorem. To make it tractable, we use $q(\x_{t-1} \mid \x_t, \x_0)$ which is conditioned on $\x_0$, which can be written as</p> \[q(\x_{t-1} \vert \x_t, \x_0) = q(\x_t\mid\x_{t-1},\x_0)\frac{q(\x_{t-1}\mid\x_0)}{q(\x_t\mid\x_0)} = \mathcal{N}(\x_{t-1}; \color{blue}{\tilde{\bm{\mu}}}(\x_t, \x_0), \color{red}{\tilde{\beta}_t} \mathbf{I}),\] <p>where</p> <p>\(\begin{align} \tilde{\beta}_t &amp;= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t \nonumber\\ \tilde{\bm{\mu}}_t (\x_t, \x_0) &amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \x_0 \nonumber\\ &amp;= \frac{1}{\sqrt{\alpha_t}} \Big( \x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon}_t \Big) = \tilde{\bm{\mu}}_t \end{align}\) with $\x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\x_t - \sqrt{1 - \bar{\alpha}_t}\bm{\epsilon}_t)$ (derived from Eq. (1)). The details of derivations can be found <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">here</a> (complete the square).</p> <p>Note that $q(\x_{t-1} \mid \x_t, \x_0) = q(\x_{t-1} \mid \x_t)$ due to Markovian property. The decoder $p_\theta$ with parameters $\theta$ is used to approximate $q(\x_{t-1} \mid \x_t, \x_0)$ with the same form as $q(\x_{t-1} \mid \x_t, \x_0)$ (Gaussian), i.e.,</p> \[p_\theta(\x_{t-1} \mid \x_{t}) = \N\nbr{\x_{t-1}\mid \bm{\mu}_\theta(\x_t, t), \bm{\Sigma}_\theta(\x_t, t)}.\] <h2 id="training-elbo">Training: ELBO</h2> <p>Our objective is to maximize the ELBO $-KL\sbr{q(\x_{1:T}\mid\x_0)\mid p_\theta(\x_{0:T})}$ which is equivalent to minimize the negative ELBO</p> \[\begin{align*} L &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{\log \frac{q(\x_{1:T}\mid\x_0)}{p_\theta(\x_{0:T})}}\\ &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{\log \frac{\prod_{t=1}^{T} q(\x_{t}\mid\x_{t-1})}{p_\theta(\x_T)\prod_{t=1}^{T} p_\theta(\x_{t-1}\mid\x_{t})}}\\ &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{-\log p_\theta(\x_T) + \sum_{t=2}^T\log \frac{q(\x_t\mid\x_{t-1})}{p_\theta(\x_{t-1}\mid\x_t)} + \log\frac{q(\x_1\mid\x_0)}{p_\theta(\x_0\mid\x_1)}}\\ &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{-\log p_\theta(\x_T) + \sum_{t=2}^T\log \nbr{\frac{q(\x_{t-1}\mid\x_{t},\x_0)}{p_\theta(\x_{t-1}\mid\x_t)}\frac{q(\x_t\mid\x_0)}{q(\x_{t-1}\mid\x_0)}} + \log\frac{q(\x_1\mid\x_0)}{p_\theta(\x_0\mid\x_1)}}\\ &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{-\log p_\theta(\x_T) + \sum_{t=2}^T\log \frac{q(\x_{t-1}\mid\x_{t},\x_0)}{p_\theta(\x_{t-1}\mid\x_t)} + \log \frac{q(\x_T\mid\x_0)}{q(\x_{1}\mid\x_0)} + \log\frac{q(\x_1\mid\x_0)}{p_\theta(\x_0\mid\x_1)}}\\ &amp;= \mathbb{E}_{\x_{1:T}\sim q(\x_{1:T}\mid\x_0)}\sbr{\log \frac{q(\x_T\mid\x_0)}{p(\x_T)} + \sum_{t=2}^T\log \frac{q(\x_{t-1}\mid\x_{t},\x_0)}{p_\theta(\x_{t-1}\mid\x_t)} - \log p_\theta(\x_0\mid\x_1)}\\ &amp;= \underbrace{KL\sbr{q(\x_T\mid\x_0)\mid p(\x_T)}}_{L_T} + \sum_{t=2}^T \underbrace{\mathbb{E}_{\x_t\sim q(\x_t\mid\x_0)}\sbr{KL\sbr{q(\x_{t-1}\mid\x_{t},\x_0)\mid p_\theta(\x_{t-1}\mid\x_t)}}}_{L_{t}} - \underbrace{\mathbb{E}_{\x_{1}\sim q(\x_{1}\mid\x_0)}\sbr{\log p_\theta(\x_0\mid\x_1)}}_{L_0} \end{align*}\] <h3 id="parameterization-on-l_t">Parameterization on $L_t$</h3> <p>For $L_t$, we assume the decoder $p_\theta$ has the same form as $q(\x_{t-1}\mid\x_t, \x_0)$ (see Eq. (2)), and the mean $\bm{\mu}_\theta$ is parameterized as</p> \[\bm{\mu}_\theta = \frac{1}{\sqrt{\alpha_t}} \nbr{\x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon}_\theta(\x_t, t)}\] \[p_\theta(\x_{t-1}\mid\x_t) = \N\nbr{\x_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \bm{\epsilon}_\theta(\x_t, t) \Big), \bm{\Sigma}_\theta(\x_t, t)}\] <p>In the above case, the $\bm{\epsilon}_\theta(\x_t, t)$ is the output of the backbone model (usually U-net) which is used to approximate the added noise $\bm{\epsilon}_t$ in the forward process.</p> <p>The KL divergence between two Gaussian:</p> \[_{KL}(p||q) = \frac{1}{2}\left[\log\frac{|\Sigma_q|}{|\Sigma_p|} - d + (\bm{\mu_p}-\bm{\mu_q})^T\Sigma_q^{-1}(\bm{\mu_p}-\bm{\mu_q}) + tr\left\{\Sigma_q^{-1}\Sigma_p\right\}\right]\] <p>We set $\bm{\Sigma}_\theta(\x_t, t) = \sigma_t^2\bm{I}$ where $\sigma_t^2 = \tilde{\beta}_t$ or $\sigma_t^2 = \beta_t$ \(\begin{align} L_t &amp;= \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\frac{1}{2\sigma_t^2} \| \tilde{\bm{\mu}}_t(\x_t, \x_0) - \bm{\mu}_\theta(\x_t, t) \|^2 } \nonumber\\ &amp;= \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \sigma_t^2} \|\bm{\epsilon}_t - \bm{\epsilon}_\theta(\x_t, t)\|^2} \nonumber\\ &amp;= \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\frac{ (1 - \alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \sigma_t^2} \|\bm{\epsilon}_t - \bm{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\x_{0} + \sqrt{1-\bar{\alpha}_{t}} \bm{\epsilon}_t, t)\|^2} \\ \end{align}\)</p> <p>Eq. (3) is further reduced to</p> <p>\(\begin{equation} L_{\text{simple}}(\theta) := \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\|\bm{\epsilon}_t - \bm{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\x_{0} + \sqrt{1-\bar{\alpha}_{t}} \bm{\epsilon}_t, t)\|^2}, \end{equation}\) where the weight term is removed for better sample quality.</p> <p>From another perspective, $\bm{\mu}_\theta$ can be parameterized as</p> \[\bm{\mu}_\theta = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \tilde{\x}_\theta(\x_t,t),\] <p>where $\tilde{\x}_\theta(\x_t,t)$ is the output of the backbone model (U-net) which is used to predict the data $\x_0$ directly.</p> <h3 id="l_t-and-l_0">$L_T$ and $L_0$</h3> <p>$L_T$ is considered a constant and ignored in the training (if $\beta_t$ is fixed). $L_0$ can be regraded as reconstruction error (VAE settings), i.e., $t=1$ in Eq. (4). More details can be found in the <a href="https://arxiv.org/abs/2006.11239">DDPM paper Sec. 3.3</a>.</p> <h2 id="implementation">Implementation</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/post_img/DDPM-algo-480.webp 480w,/assets/post_img/DDPM-algo-800.webp 800w,/assets/post_img/DDPM-algo-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/post_img/DDPM-algo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Traning and sampling process (Source: DDPM paper) </div> <h1 id="connection-with-ddim">Connection with DDIM</h1> <p>DDIM is proposed to accelerate the inference of DDPM. The formulation is slightly different but the training is proved to be the same.</p> <p>The forward process is non-Markovian. Consider a family $Q$, indexed by a real vector $\sigma\in\R^T$, the joint is now conditioned on $\x_0$: $q_\sigma(\x_{1:T}\mid\x_0) = q_\sigma(\x_T\mid\x_0)\prod_{i=2}^Tq_\sigma(\x_{t-1}\mid\x_{t},\x_0)$ where</p> \[\begin{equation} q_\sigma(\x_{t-1}\mid\x_{t},\x_0) = \N\nbr{\sqrt{\bar{\alpha}_{t-1}}\x_0 + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2}\frac{\x_t-\sqrt{\bar{\alpha}_{t}}\x_0}{\sqrt{1-\bar{\alpha}_{t}}},\sigma_t^2\bm{I}} \end{equation}\] <p>The Eq. (4) is selected to satisfy the joint and $q_\sigma(\x_t\mid\x_0) = \N\nbr{\sqrt{\bar{\alpha}_t}, (1-\bar{\alpha}_t)\bm{I}}$.</p> <p>The forward process is obtained by $q_\sigma(\x_t\mid\x_{t-1},\x_0) = \frac{q_\sigma(\x_{t-1}\mid\x_{t},\x_0)q_\sigma(\x_t\mid\x_0)}{q_\sigma(\x_{t-1}\mid\x_0)}$. It is also Gaussian and the magnitude controls the randomness in the forward process. If we set $\sigma_t = 0$, the forward process becomes deterministic.</p> <p>The reverse process is similar to the DDPM. The joint $p_\theta(\x_{0:T})$ is used to approximate the $q_\sigma(\x_{t-1}\mid\x_{t},\x_0)$ by minimization of KL-divergence and the training objective is the same as $L_{\text{simple}}$ of DDPM (<a href="https://arxiv.org/abs/2010.02502">Theorem 1 in DDIM paper</a>). We can let the model (U-net) predict either $\x_0$ directly or the noise $\bm{\epsilon}_t$. For example (noise prediction),</p> \[f_\theta(\x_t) = \frac{\x_t-\sqrt{1-\bar{\alpha}_t}\bm{\epsilon}_{\theta}(\x_t,t)}{\sqrt{\bar{\alpha}_t}} = \tilde{\x}_0\] <p>According to Eq. (5),</p> \[\begin{align} \x_{t-1} &amp;= \sqrt{\bar{\alpha}_{t-1}}\tilde{\x}_0 + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2}\frac{\x_t-\sqrt{\bar{\alpha}_{t}}\tilde{\x}_0}{\sqrt{1-\bar{\alpha}_{t}}} + \sigma_t\bm{\epsilon}\\ &amp;= \sqrt{\bar{\alpha}_{t-1}}\nbr{\frac{\x_t-\sqrt{1-\bar{\alpha}_t}\bm{\epsilon}_\theta(\x_t,t)}{\sqrt{\bar{\alpha}_{t}}}} + \sqrt{1-\bar{\alpha}_{t}-\sigma_t^2}\bm{\epsilon}_\theta(\x_t,t) + \sigma_t\bm{\epsilon} \end{align}\] <p>$\sigma_t$ is set to $0$ in DDIM and the forward process becomes deterministic and the reverse process becomes implicit probabilistic model.</p> <h2 id="accelerated-inference">Accelerated Inference</h2> <p>In DDPM, we need to go through every forward steps to sample in backward process due to Markovian property. DDIM proposes to use a subset of total time steps to sample in reverse process, i.e., ${\x_{\tau_1},…,\x_{\tau_S}}\subset{\x_1,…,\x_T}$. From another perspective, the interval between two time steps is increased in the reverse process, e.g., $\tau_{s+1}-\tau_s = \Delta\tau \geq 1$ where $\Delta\tau$ is a hyperparameter. Now, we are sampling from $q_\sigma(\x_{t-\Delta_\tau}\mid\x_{t},\x_0)$ iteratively. Hence, the reverse process is accelerated. The reason we can do this is that the loss function does depend on the “marginal” $q_\sigma(\x_t\mid\x_0)$ but not directly on the joint $q_\sigma(\x_{1:T}\mid\x_0)$. Also, the backward process is non-Markovian, i.e., $q_\sigma(\x_{t-\Delta_\tau}\mid\x_{t},\x_0)$. Hence, we can consider a subset of latent variables instead of all of them.</p> <h1 id="connection-with-score-based-dm">Connection with Score-based DM</h1> <h2 id="score-and-score-matching">Score and Score Matching</h2> <p>In order to learn a distribution $p_{data}$, we need to represent it first. Usually, we define</p> \[p_\theta(\x) = \frac{e^{-f_\theta(\x)}}{Z_\theta}\] <p>where it satisfies $p_\theta(\x) \geq 0$ and $\int p_\theta(\x)d\x = 1$. For a dataset ${\x_1,…,\x_N}$, we cannot use MLE to estimate the parameters $\theta$ because the likelihood is intractable due to the normalizing factor $Z_\theta$. Instead, we can use the score function to estimate the parameters. Score of a probability density function $p(\x)$ is defined as $\nabla_{\x}\log p(x)$. For example, if $p$ is Gaussian distributed, $\nabla_{\x}\log p(\x) = -\frac{\x-\bm{\mu}}{\sigma^2} = -\frac{\bm{\epsilon}}{\sigma}$ (standardization). More details can be found in <a href="https://arxiv.org/abs/2101.03288">How to Train Your Energy-Based Models</a>.</p> <p>We want to learn a score model $\bm{s}_\theta(\x)=\nabla_{\x}\log p_\theta = -\nabla_{\x}f_{\theta}(\x)\approx \nabla_{\x}\log p_{data}$. Then, we can draw samples from the approximated score function. Can we draw samples directly from score function? Yes, <strong>Langevin Dynamics</strong>. To train $\bm{s}_\theta$, we minimize the Fisher Divergence, which is also called score matching,</p> \[\begin{equation} D_F\sbr{p_{data}\mid p_\theta} = \mathbb{E}_{p_{data}}\sbr{\frac{1}{2}\left\lVert\nabla_{\x}\log p_{data}(\x) - \bm{s}_\theta(\x)\right\rVert_2^2}. \end{equation}\] <p>However, $p_{data}$ is unknown. Fortunately, it can be replaced by the derivative of $\bm{s}_\theta(\x)$ to eliminate the unknown $p_{data}$ (see the derivation from <a href="https://jmlr.org/papers/v6/hyvarinen05a.html">Estimation of Non-Normalized Statistical Models by Score Matching</a>),</p> \[\begin{equation} D_F\sbr{p_{data}\mid p_\theta} = \mathbb{E}_{p_{data}}\sbr{Tr(\nabla_{\x} \bm{s}_\theta(\x)) + \frac{1}{2}\left\lVert \bm{s}_\theta(\x)\right\rVert_2^2} + \text{Constant}. \end{equation}\] <p>One critical problem is that the score matching is not scalable to deep networks and high dimensional data due to the derivative term.</p> <h2 id="ddpm-to-score-matching-tweedies-formula">DDPM to Score Matching (Tweedie’s Formula)</h2> <p>From Eq. (1), we have the forward process $\x_t\sim\N(\x_t \mid \sqrt{\bar{\alpha}_t}\x_0, (1-\bar{\alpha}_t)\I)$. By Tweedie’s formula, the mean can be approximated as</p> \[\begin{align*} \sqrt{\bar{\alpha}_t}\x_0 &amp;= \x_t + (1-\bar{\alpha}_t)\nabla_{\x_t}\log p(\x_t)\\ \x_0 &amp;= \frac{\x_t}{\sqrt{\bar{\alpha}_t}} + \frac{1-\bar{\alpha}_t}{\sqrt{\bar{\alpha}_t}}\nabla_{\x_t}\log p(\x_t).\\ \end{align*}\] <p>In this case, the $\tilde{\bm{\mu}}_t (\x_t, \x_0)$ can be parameterized as</p> \[\tilde{\bm{\mu}}_t (\x_t, \x_0) = \frac{1}{\sqrt{\alpha}_t} \x_t + \frac{1-\alpha_t}{\sqrt{\alpha}_t}\nabla_{\x_t}\log p(\x_t).\] <p>Then, the backbone model is used to predict the score at time step $t$. The approximation becomes</p> \[\bm{\mu}_\theta = \frac{\x_t}{\sqrt{\bar{\alpha}_t}} + \frac{1-\bar{\alpha}_t}{\sqrt{\bar{\alpha}_t}}\bm{s}_\theta(\x_t, t).\] <p>Finally, the objective becomes</p> \[\begin{align*} L_t &amp;= \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\frac{1}{2\sigma_t^2} \| \tilde{\bm{\mu}}_t(\x_t, \x_0) - \bm{\mu}_\theta(\x_t, t) \|^2 }\\ &amp;= \mathbb{E}_{\x_0, \bm{\epsilon}} \sbr{\frac{1}{2\sigma_t^2}\frac{ (1 - \alpha_t)^2 }{\alpha_t} \|\bm{s}_\theta(\x_t, t) - \nabla_{\x_t}\log p(\x_t)\|^2}. \end{align*}\] <p>Note that the true noise and the score looks very similar in the above equation. If we compare two type of $\x_0$ from score and noise, we have $\nabla_{\x_t}\log p(\x_t)=-\frac{1}{\sqrt{1-\bar{\alpha}}}\bm{\epsilon}$.</p> <h2 id="guidance-from-score">Guidance from Score</h2> <p>In guided diffusion models, we approximate the conditional data distribution $p(\x\mid y)$ instead of $p(\x)$. Hence, our goal is to learn the gradient $\nabla_{\x_t}\log p(\x_t\mid y)$ which can be written as</p> \[\begin{equation} \nabla_{\x_t}\log p(\x_t\mid y) = \underbrace{\nabla_{\x_t}\log p(y\mid\x_t)}_{\text{adversarial gradient}} + \underbrace{\nabla_{\x_t}\log p(\x)}_{\text{uncond. gradient}}. \end{equation}\] <h3 id="classifier-guidance">Classifier Guidance</h3> <p>In the classifier guidance, we have a classifier $\bm{C}_\phi$ which is used to predict the label $y$ given noised data $\x_t$. The gradient of the classifier is used as guidance, i.e.,</p> \[\begin{align*} \nabla_{\x_t}\log p(\x_t\mid y) &amp;\approx \nabla_{\x_t}\log C_\phi(y\mid\x_t) + \bm{s}_\theta(\x_t, t)\\ &amp;= \nabla_{\x_t}\log C_\phi(y\mid\x_t) - \frac{1}{\sqrt{1-\bar{\alpha}}}\bm{\epsilon}_\theta(\x_t, t)\\ &amp;= - \frac{1}{\sqrt{1-\bar{\alpha}}} \underbrace{\nbr{\bm{\epsilon}_\theta(\x_t, t) - \sqrt{1-\bar{\alpha}}\nabla_{\x_t}\log C_\phi(y\mid\x_t)}}_{\text{new predictor}}. \end{align*}\] <p>Note that $\nabla_{\x_t}\log p(\x_t)=-\frac{1}{\sqrt{1-\bar{\alpha}}}\bm{\epsilon}$. Then, the new predictor can be combined with a parameter $w$ to control the guidance strength, i.e.,</p> \[\begin{equation} \tilde{\bm{\epsilon}}(\x_t, t) = \bm{\epsilon}_\theta(\x_t, t) - w\sqrt{1-\bar{\alpha}}\nabla_{\x_t}\log C_\phi(y\mid\x_t), \end{equation}\] <p>where we have the form</p> \[\nabla_{\x_t}\log p(\x_t) + \gamma \nabla_{\x_t}\log p(y\mid\x_t).\] <p>One of the drawbacks of the classifier guidance is that the classifier has to be trained separately. In most of work, $\bm{\epsilon}_\theta(\x_t, t)$ is replaced by the conditioned version $\bm{\epsilon}_\theta(\x_t, t, y)$ in Eq. (11).</p> <h3 id="classifier-free-guidance">Classifier-free Guidance</h3> <p>To avoid the classifier, we need to reconsider the term $\nabla_{\x_t}\log p(y\mid\x_t)$. With Bayes’ theorem, we have</p> \[\begin{align*} \nabla_{\x_t}\log p(y\mid\x_t) &amp;= \nabla_{\x_t}\log p(\x_t\mid y) - \nabla_{\x_t}\log p(\x_t)\\ &amp;\approx \bm{s}_\theta(\x_t, t, y) - \bm{s}_\theta(\x_t, t)\\ &amp;= -\frac{1}{\sqrt{1-\bar{\alpha}}}\nbr{\bm{\epsilon}_\theta(\x_t, t, y)-\bm{\epsilon}_\theta(\x_t, t)} \end{align*}\] <p>The idea is that we use a new model $\bm{\epsilon}_\theta(\x_t, t, y)$ to learn the conditioned score. To obtain the approximated $\nabla_{\x_t}\log p(\x_t\mid y)$, we just add the unconditioned score, i.e.,</p> \[\begin{align*} \nabla_{\x_t}\log p(\x_t\mid y) &amp;\approx \gamma\nabla_{\x_t}\log p(y\mid\x_t) + \nabla_{\x_t}\log p(\x_t)\\ &amp;\approx -\frac{\gamma}{\sqrt{1-\bar{\alpha}}}\nbr{\bm{\epsilon}_\theta(\x_t, t, y)-\bm{\epsilon}_\theta(\x_t, t)} - \frac{1}{\sqrt{1-\bar{\alpha}}}\bm{\epsilon}_\theta(\x_t, t)\\ &amp;=- \frac{1}{\sqrt{1-\bar{\alpha}}}\underbrace{\nbr{\gamma\bm{\epsilon}_\theta(\x_t, t, y) - (\gamma-1)\bm{\epsilon}_\theta(\x_t, t)}}_{\text{new predictor}}. \end{align*}\] <p>If we set $\gamma=w+1$, we have the same form as the one in the classifier-free paper. Hence, the new predictor can be written as</p> \[\begin{equation} \tilde{\bm{\epsilon}}(\x_t, t, y) = (w+1)\bm{\epsilon}_\theta(\x_t, t, y) - w\bm{\epsilon}_\theta(\x_t, t). \end{equation}\] <p>The advantage of the classifier-free guidance is that we do not need to train the classifier separately. The conditioned model $\bm{\epsilon}_\theta(\x_t, t, y)$ and unconditioned model $\bm{\epsilon}_\theta(\x_t, t, \emptyset)$ are learned jointly by “turn off” a certain ratio of the labels (looks like dropout normalization). For example, if we set the ratio (threshold) to $p$, we generate a mask <code class="language-plaintext highlighter-rouge">mask = torch.rand(cemb.shape[0])&lt;threshold</code> where <code class="language-plaintext highlighter-rouge">cemb</code> is a batch of label embeddings. Then, we “turn off” these labels, i.e., <code class="language-plaintext highlighter-rouge">cemb[np.where(mask)[0]] = 0</code>. Finally, we add time embeddings and label embeddings, i.e., <code class="language-plaintext highlighter-rouge">emb = cemb + temb</code> to as the input to the backbone model.</p> <h1 id="noise-conditional-score-networks-ncsn">Noise Conditional Score Networks (NCSN)</h1> <p>In <a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a>, the authors propose a score-based model called Noise Conditional Score Networks (NCSN). To circumvent the direct computation of $Tr(\nabla_{\x}^2\log p_{\bm{\theta}}(\x))$, <strong>Denoising Score Matching</strong> or <strong>Sliced Score Matching</strong> are proposed. Here, we focus on the former. The idea is that we create a data distribution that very close to the true data distribution by perturbing the data $\x$ with a pre-specified noise distribution $q_{\sigma}(\tilde{\x}\mid\x)$ and the perturbed distribution is $q(\tilde{\x}) := \int p_{data}(x)q_{\sigma}(\tilde{\x}\mid\x)d\x$. To learn the score of data distribution, we minimize the Fisher divergence</p> \[\begin{align} D_F\sbr{q(\tilde{\x})\mid p_\theta(\tilde{\x})} &amp;= \mathbb{E}_{q_{\sigma}(\tilde{\x})}\mathbb{E}_{p_{data}(\x)}\sbr{\frac{1}{2}\left\lVert\bm{s}_\theta(\tilde{\x}) - \nabla_{\tilde{\x}} q_{\sigma}(\tilde{\x})\right\rVert^2} \\ &amp;= \mathbb{E}_{q_{\sigma}(\tilde{\x}\mid\x)}\mathbb{E}_{p_{data}(\x)}\sbr{\frac{1}{2}\left\lVert\bm{s}_\theta(\tilde{\x}) - \nabla_{\tilde{\x}} q_{\sigma}(\tilde{\x}\mid\x)\right\rVert^2 } + C \end{align}\] <p>Note that $\bm{s}_{\theta^*}(\tilde{\x})=\nabla_{\x}q_\sigma(\tilde{\x})$ almost surely by minimizing Eq. (14). However, $\bm{s}_{\theta^*}(\tilde{\x}) = \nabla_{\tilde{\x}} q_{\sigma}(\tilde{\x}) \approx \nabla_{\x}\log p_{data}(\x)$ is true only when the noise level $\sigma$ is small enough, which leads to $q_{\sigma}(\tilde{\x}) \approx p_{data}(\x)$.</p> <p>There are two problems in score matching:</p> <ul> <li><strong>Inaccurate score estimation in low data density regions:</strong> the data often concentrate on low dimensional manifolds embedded in a high dimensional space (a.k.a., the ambient space). It means that the data don’t cover the whole $\mathbb{R}^D$ space. Hence, the score estimation is inaccurate in the low data density regions. Since the sampling is a iterative process, the inaccurate score estimation will lead to a biased sampling process.</li> <li><strong>Slow mixing of Langevin dynamics:</strong> sampling process may be very slow if the distribution has 2 modes and they are separated by a low density region. The Langevin dynamics may be trapped in the low density region and cannot move to the other mode.</li> </ul> <p>In NCSN, data are perturbed by different levels of noise. Let $\sigma_1&gt;\sigma_2&gt;…&gt;\sigma_L$ be the noise levels which is a positive geometric sequence. The perturbed data distribution is written as $q_{\sigma_i}(\tilde{\x})=\int p_{data}(\x)p(\tilde{\x} \mid \x)d\x = \int p_{data}(\x)\N(\tilde{\x}\mid \x, \sigma_i) d\x $. We choose the noise function $q_{\sigma_i}(\tilde{\x} \mid \x) = \N(\x,\sigma_i^2\I)$ and $\nabla_{\tilde{\x}}\log q_{\sigma_i}(\tilde{\x}\mid \x) = -\frac{\tilde{\x}-\x}{\sigma_i^2}$. Intuitively, the perturbed data with different noise levels fill the low density regions. It may be regarded as a data augmentation technique. Also, these perturbed data build a “tunnel” between the true data distribution and the prior distribution ($\N(0,I)$), which helps improve the mixing rate of Langevin dynamics on multimodal distributions.</p> <p>For a given noise level $\sigma_i$, the objective is</p> \[\ell_i(\theta;\sigma_i) = \mathbb{E}_{q_{\sigma_i}}\mathbb{E}_{p_{data}}\sbr{\frac{1}{2}\left\lVert\bm{s}_\theta(\tilde{\x},\sigma_i) + \frac{\tilde{\x}-\x}{\sigma_i^2}\right\rVert^2_2}.\] <p>Then, the total objective is</p> \[\mathcal{L}(\theta) = \sum_{i=1}^L\lambda(\sigma_i)\ell_i(\theta;\sigma_i).\] <p>The author found that $\left\lVert\bm{s}_\theta(\tilde{\x},\sigma_i)\right\rVert_2\propto \frac{1}{\sigma_i}$. To balance the contribution of each noise level, the weight $\lambda(\sigma_i)$ is set to $\sigma_i^2$ to make the objective scale-invariant $\frac{\tilde{\x}-\x}{\sigma_i}\sim \N(0,\I)$.</p> <h2 id="langevin-dynamics-sde">Langevin Dynamics (SDE)</h2> <p>It can be represented as Ito’s diffusion</p> \[\begin{aligned} d\x_t &amp;= -\nabla_{\x}V(\x)dt + \sqrt{2}d\bm{w}_t \\ &amp;= -\nabla_{\x}V(\x)dt + \sqrt{2dt}\bm{z}_t, \quad \bm{z}_t\sim\N(0,\I). \end{aligned}\] <p>where $\bm{w}_t\sim\N(0,\bm{t})$ is the standard Wiener process and $V(\x)$ is the potential energy. The steady state distribution of the above SDE can be determined by the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Plank equation</a> which is a partial differential equation (PDE) that describes the evolution of a probability distribution over time under the effect of drift forces and random (or noise) forces. To be more specific, we want to find the stationary distribution $p(\x)$ that satisfies the following equation</p> \[\frac{\partial p(\x,t)}{\partial t} = \frac{\partial }{\partial \x}\Big(\frac{\partial V(\x)}{\partial \x}p(\x,t)\Big) + \frac{\partial^2 p(\x,t)}{\partial \x^2} = 0\] <p>which has the equilibrium distribution $p(\x) \propto e^{-V(\x)}$. We hope that the distribution $p(\x)$ is close to the true data distribution $p_{data}(\x)$ in our case. Recall that $p_\theta(\x) = \frac{e^{-f_\theta(\x)}}{Z_\theta}$, we can set $V(\x) = f_\theta(\x)$ and then $\nabla_{\x}V(\x) = \bm{s}_\theta(\x)$.</p> <h3 id="sampling">Sampling</h3> <p>The discretization of the above SDE can be written as</p> \[\x_{t+\epsilon} = \x_{t} - \epsilon\bm{s}_{\theta}(\x) + \sqrt{2\epsilon}\bm{z}_t.\] <p>The sampling can be done by iteratively updating the data $\x$ with the above equation. In NCSN, the sampling process is slightly modified by using different step length at each noise level (annealed Langevin dynamics). The step length is set to $\alpha_i = \epsilon \frac{\sigma_i}{\sigma_L}$. The sampling process is</p> \[\x_{t} \leftarrow \x_{t-1} + \frac{\alpha_i}{2}\bm{s}_\theta(\x,\sigma_i) + \sqrt{\alpha_i}\bm{z}_i \quad t=0,1,...,T, \quad i=1,...,L.\] <p>Note that there are 2 loops in the sampling process. The outer loop is for the noise level and the inner loop is for the Langevin dynamics.</p>]]></content><author><name></name></author><category term="Bayes"/><category term="Deep-Learning"/><category term="Diffusion-Model"/><summary type="html"><![CDATA[basics of diffusion models]]></summary></entry><entry><title type="html">Revisit of Local Reparameterization Trick</title><link href="https://soloche.github.io/blog/2022/LRT/" rel="alternate" type="text/html" title="Revisit of Local Reparameterization Trick"/><published>2022-12-26T15:00:00+00:00</published><updated>2022-12-26T15:00:00+00:00</updated><id>https://soloche.github.io/blog/2022/LRT</id><content type="html" xml:base="https://soloche.github.io/blog/2022/LRT/"><![CDATA[<h1 id="brief-introduction">Brief introduction</h1> <p>Paper can be found <a href="https://arxiv.org/pdf/1506.02557.pdf">here</a> [1]. This paper proposed <strong>local reparameterization technique</strong> to reduce the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability.</p> <p>Notation: data $\mathcal{D} = \{\nbr{\x_i, y_i}\}_{i=1}^N$ , a single data $\mathcal{D^i} = \{(\x_i, y_i)\}$</p> <h5 id="todo">Todo:</h5> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Introduction</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Local Reparameterization Trick</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked"/>Variance of Gradient</li> </ul> <h2 id="variance-of-the-sgvb-estimator">Variance of the SGVB estimator</h2> <p>Recall Bayesian framework $P(\w|\mathcal{D}) = \frac{P(\mathcal{D}\mid\w) P(\w)}{P(\mathcal{D})}$ and variational posterior $q(\w\mid\mathcal{D})$. Recall the loss function negative ELBO:</p> \[\begin{align} L(\theta;\mathcal{D}) &amp;= \underbrace{KL[q(\w\mid\theta)\mid P(\w)]}_{\text{complexity cost}} - \underbrace{\mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]}_{\text{likelihood cost}}\\ \end{align}\] <p>We assume that the complexity cost can be calculated analytically and the likelihood requires approximations. Hence, the variance is from the likelihood term which can be written as</p> \[\begin{align} \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)] &amp;= \mathbb{E}_{q(\w\mid\mathcal{D})}\sbr{\sum_{i=1}^N\log P(\mathcal{D^i\mid\w})}\\ &amp;\approx\sum_{i=1}^{N}\sbr{\frac{1}{n}\sum_{j=1}^n\log P(\mathcal{D^i\mid\w^j})}\\ &amp;\approx\sum_{i=1}^{N}\log P(\mathcal{D^i\mid\w}) \quad \text{for $n=1$} \end{align}\] <p>where we use $n$ samples for each data $\mathcal{D}^i$ and there will be $N\times n$ samples of $\w$. If $n = 1$, Eq. (4) is the summation of log density of likelihood.</p> <p>To construct the expected likelihood of the full data based on minibatches with size $M$, Eq. (3) is further written as</p> \[L_{M} = \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]\approx \frac{N}{M}\sum_{i=1}^{M}\log P(\mathcal{D^i\mid\w}),\] <p>where we set $n = 1$. In VAE paper [2], it says that if the batch size $M$ is large enough, $n$ could be $1$. The variance of $L_M$ can be decomposed as</p> \[\begin{align} Var[L_M] &amp;= \frac{N^2}{M^2}\nbr{\sum_{i=1}^{M}Var[L_i] + 2\sum_{i=1}^{M}\sum_{j=i+1}^{M}Cov[L_i,L_j]}\nonumber \\ &amp;= N^2\nbr{\frac{1}{M}Var[L_i] + \frac{M-1}{M}Cov[L_i, L_j]}, \end{align}\] <p>where $L_M = \frac{N}{M}\sum_{i=1}^{M}L_i$ and $L_i = \log P(\mathcal{D^i\mid\w})$. The problem here is the coefficient $\frac{M-1}{M}$ ahead of covariance. It is not inversely proportional to $M$ as the variance term. Our target now is to remove the covariance, i.e., $Cov[L_i, L_j] = 0.$</p> <h2 id="local-reparameterization-trick">Local reparameterization trick</h2> <p>Some notations:</p> <ul> <li>$\bm{A}: M \times 1000$ feature matrix with $M$ samples</li> <li>$\bm{W}: 1000 \times 1000$ weight matrix with the element $w_{i,j} \sim \N(\mu_{i,j}, \sigma^{2}_{i,j})$</li> <li>$\bm{B} = \bm{AW}$: $M \times 1000$ output before activation</li> </ul> <p>With the reparameterization trick, $w_{i,j} = \mu_{i,j} + \sigma_{i,j}\epsilon_{i,j}$ where $\epsilon_{i,j} \sim \N(0,1)$. To make $Cov[L_i, L_j] = 0$, we can sample $\w^1,…,\w^M$ for each observation in the minbatch because $\w^1,…,\w^M$ are i.i.d. and the randomness of the likelihood is only from the weight $\w$. However, it requires $M \times 1000 \times 1000$ samples for the minibatch only in a single layer.</p> <p>To address the issue, we can sample $\bm{B}$ directly. $\bm{B=AW}$ where $\w \sim \N(\bm{\mu}, \bm{\Sigma})$ can be regarded as a sequence of linear combination of Gaussian random variables which is still Gaussian:</p> \[\bm{B} \sim \N(\bs{A\mu},\bs{A\Sigma A^T})\] <p>For each element $b_{m,j}$:</p> \[\begin{align} \gamma_{m,j} &amp;= \sum_{i=1}^{1000}a_{m,i}\mu_{i,j}\\ \delta_{m,j} &amp;= \sum_{i=1}^{1000}a^2_{m,i}\sigma^2_{i,j} \end{align}\] <p>To sample from $b_{m,j} = \gamma_{m,j} + \sqrt{\delta_{m,j}}\zeta_{m,j}$ where $\zeta_{m,j} \sim \N(0,1)$. In this case, the number of samples is $M \times 1000$.</p> <h2 id="variance-of-gradient">Variance of gradient</h2> <p>The local reparameterization trick also leads to lower variance of gradient compared to the method that sample $\w$ for each data. The gradient of posterior variance $\sigma^2_{i,j}$ with and without local reparameterization trick in Eq. (8) and (9):</p> \[\begin{align} \frac{\partial L_M}{\partial \sigma^2_{i,j}} &amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\partial b_{m,j}}{\partial \delta_{m,j}}\frac{\partial \delta_{m,j}}{\partial \sigma^2_{i,j}}\nonumber \\ &amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\zeta_{m,j}}{2\sqrt{\delta_{m,j}}}a^2_{m,i}\\ \end{align}\] \[\begin{equation} \frac{\partial L_M}{\partial \sigma^2_{i,j}} = \frac{\partial L_M}{\partial b_{m,j}}\frac{\epsilon_{i,j}}{2\sigma_{i,j}}a_{m,i} \end{equation}\] <p>The two sources of randomness are $b_{m,j}$ and $\zeta_{m,j}$ for Eq. (8), $b_{m,j}$ and $\epsilon_{i,j}$ for Eq. (9). We may notice the difference of the subscript between random variable $\epsilon$ and $\zeta$. One intuitive explanation that Eq. (8) has lower variance than Eq. (9) is that there is more than one random variables $\epsilon$ contributes to $b_{m,j}$ in Eq. (9), i.e., $\epsilon_{i,j}$ for $i = 1,…,1000$ in this case. In contrast, only one variable $\zeta_{m,j}$ contributes to $b_{m,j}$ in Eq. (9). The mathematical analysis can be found in Appendix D from [1]. It decomposes the variance by total variance law and conditioning on $b_{m,j}$.</p> <h1 id="references">References</h1> <p>[1] Kingma, Durk P., Tim Salimans, and Max Welling. “Variational dropout and the local reparameterization trick.” Advances in neural information processing systems 28 (2015).</p> <p>[2] Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013).</p>]]></content><author><name></name></author><category term="Bayes"/><category term="Deep-Learning"/><summary type="html"><![CDATA[a brief review of paper Local Reparameterization Trick]]></summary></entry><entry><title type="html">Revisit of Bayes by Backprop</title><link href="https://soloche.github.io/blog/2022/BBP_review/" rel="alternate" type="text/html" title="Revisit of Bayes by Backprop"/><published>2022-12-10T00:00:00+00:00</published><updated>2022-12-10T00:00:00+00:00</updated><id>https://soloche.github.io/blog/2022/BBP_review</id><content type="html" xml:base="https://soloche.github.io/blog/2022/BBP_review/"><![CDATA[<h1 id="review-of-weight-uncertainty-in-neural-network">Review of Weight Uncertainty in Neural Network</h1> <p>Some useful links:</p> <ol> <li><a href="https://arxiv.org/abs/1505.05424">Paper can be found here</a></li> <li><a href="https://github.com/JavierAntoran/Bayesian-Neural-Networks">Implementation by Pytorch on Github</a></li> </ol> <h1 id="bayes-by-backprop-bbp-framework">Bayes by Backprop (BBP) Framework</h1> <p>A probabilistic model: $P(y\mid\x, \w)$: given an input $x\in \mathbb{R}^p, y\in\mathcal{Y}$, using the set of parameters or weights $\w$.</p> \[P(\w\mid\x, y) = \frac{P(y\mid\x, \w)p(\w)}{P(y)}\] <p>Note that we assume the each parameter $w_i$ are i.i.d. and then we have $p(\w)=p(w_1)p(w_2)…p(w_m)$ where $m$ is the number of parameters.</p> <h1 id="loss-function">Loss Function</h1> <p>The weights can be learnt by MLE given a set of training samples $\mathcal{D} = \{\x_i, y_i\}_i$</p> \[\begin{align*} \w^{MLE} &amp;= \argmax{w} \log P(\mathcal{D}\mid\w)\\ &amp;= \argmax{w} \log \sum_i^n P(y_i\mid\x_i,\w) \end{align*}\] <p>Regularization can be done by add a prior on the weights $w$ and finding the MAP, i.e.,</p> \[\begin{align*} \w^{MAP} &amp;= \argmax{w} \log P(\w\mid\mathcal{D})\\ &amp;= \argmax{w} \log P(\mathcal{D}\mid\w)p(\w) \end{align*}\] <p>Inference is intractable because it needs to consider each configuration of $w$.</p> \[P(\hat{y}\mid\hat{\x}) = \mathbb{E}_{p(\w\mid D)}[P(\hat{y}\mid\hat{\x},\w)]\] <h2 id="minimization-of-kl-divergence-elbo">Minimization of KL Divergence (ELBO)</h2> \[\begin{align*} \theta^* &amp;= \argmin{\theta} KL[q(\w\mid\theta)\mid P(\w\mid\mathcal{D})]\\ &amp;= \argmin{\theta} \mathbb{E}_{q(\w\mid\theta)}[\log q(\w\mid\theta) - \log P(\w\mid\mathcal{D})]\\ &amp;= \argmin{\theta} \mathbb{E}_{q(\w\mid\theta)}[\log q(\w\mid\theta) - \log P(\w,\mathcal{D})] + \log P(\mathcal{D})\\ &amp;= \argmin{\theta} \mathbb{E}_{q(\w\mid\theta)}[\log q(\w\mid\theta) - \log P(\mathcal{D}\mid\w) - \log P(\w)] + \log P(\mathcal{D})\\ &amp;= \argmin{\theta} KL[q(\w\mid\theta)\mid P(\w)] - \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)] + \log P(\mathcal{D})\\ &amp;= \argmin{\theta} \underbrace{KL[q(\w\mid\theta)\mid P(\w)]}_{\text{complexity cost}} - \underbrace{\mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]}_{\text{likelihood cost}} = -\text{ELBO} \end{align*}\] <p>We can find the relation $\log P(\mathcal{D}) - KL[q(\w\mid\theta)\mid P(\w\mid\mathcal{D})] = \text{ELBO}$ where $P(\mathcal{D}) = \int P(\w\mid\mathcal{D})P(\w)d\w$ is model evidence which is usually intractable and $\log P(\mathcal{D})$ is not related to the parameter $\theta$. Hence, minimization of KL divergence is equivalent to maximization of ELBO, which let us avoid computing the model evidence.</p> <p>We denote it as:</p> \[\begin{align} \mathcal{F(\mathcal{D},\theta)} &amp;= KL[q(\w\mid\theta)\mid P(\w)] - \mathbb{E}_{q(\w\mid \theta)}[\log P(\mathcal{D}\mid\w)]\\ &amp;\approx \frac{1}{n} \sum_{i=1}^n \log q(\w^i\mid\theta) - \log P(\w^i) - \log P(\mathcal{D}\mid\w^i) \end{align}\] <p>where $\w^i$ is a sample from variational posterior $q(\w\mid\theta)$ . Note that the parameters require gradient is $\theta$ instead of $\w$ in MLE or MAP. In the original paper, there is no $\frac{1}{n}$. I think it is probably a typo. The calculation of complexity cost can be done by either closed form or sample-based method.</p> <p>The factorization of complexity cost:</p> \[\begin{align*} KL[q(\w\mid\theta)\mid P(\w)] = \sum_{i=1}^m KL[q(w_i\mid\theta)\mid P(w_i)] \end{align*}\] <h2 id="unbiased-monte-carlo-gradients">Unbiased Monte Carlo Gradients</h2> <blockquote> <p>Proposition 1. Let $\epsilon$ be a random variable having probability density given by $q(\epsilon)$ and let $w = t(\theta, \epsilon)$ where $t$ is a deterministic function. Suppose further that the marginal probability density of $\w$, $q(\w\mid\theta)$, is such that $q(\epsilon)d\epsilon = q(\w\mid\theta)d\w$. Then fora a function $f$ with derivative in $\w$: \(\frac{\partial}{\partial\theta}\mathbb{E}_{q(w\mid\theta)}[f(w, \theta] = \mathbb{E}_{q(\epsilon)}[\frac{\partial f(\w, \theta)}{\partial \w}\frac{\partial \w}{\partial \theta} + \frac{\partial f(\w, \theta)}{\partial \theta}]\)</p> </blockquote> <p>Our objective function in Eq. (1) can be written as</p> \[\begin{align*} \mathcal{F(\mathcal{D},\theta)} &amp;= \mathbb{E}_{q(\w\mid\theta)}[\log q(\w\mid\theta) - \log P(\w) - \log P(\mathcal{D}\mid\w)]\\ &amp;= \mathbb{E}_{q(\w\mid\theta)}[f(\w, \theta)] \end{align*}\] <p>We need the gradient</p> \[\begin{align*} \nabla_\theta\mathbb{E}_{q(\w\mid\theta)}[f(\w, \theta)] &amp;= \nabla_\theta \int f(\w,\theta)q(\w\mid\theta)d\w\\ &amp;= \nabla_\theta \int f(w,\theta)q(\epsilon)d\epsilon\\ &amp;= \mathbb{E}_{q(\epsilon)}[\nabla_\theta f(\w,\theta)] \end{align*}\] <p>Leibniz integral rule can also used here.</p> <p><strong>Why re-parameterization trick?</strong></p> <p>Some useful links:</p> <ol> <li><a href="https://pillowlab.wordpress.com/2020/12/20/monte-carlo-gradient-estimators/">Introduction about MC gradient</a></li> <li><a href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/">Why re-parameterization trick</a></li> </ol> <p>Normally, we use Monte Carlo (MC) gradient estimator to approximate the gradient. It is used to solve the problem \(\nabla_\theta \mathbb{E}_{q(\w)}[f(\w, \theta)] = \mathbb{E}_{q(\w)}[\nabla_\theta f(\w, \theta)]\). <strong>Note that there is no parameter $\theta$ in the expectation distribution $q(w)$, which is the difference between this case and the case we are facing.</strong></p> <p>If we still use MC gradient estimator on $\nabla_\theta \mathbb{E}_{q(\w\mid\theta)}[f(\w, \theta)]$:</p> \[\begin{align*} \nabla_\theta \mathbb{E}_{q(\w\mid\theta)}[f(\w, \theta)] &amp;= \int \nabla_\theta [q(\w\mid\theta)f(\w, \theta)]d\w\\ &amp;= \int \nabla_\theta [q(\w\mid\theta)]f(\w, \theta)d\w + \int q(\w\mid\theta) \nabla_\theta[f(\w, \theta)]d\w\\ &amp;= \int q(\w\mid\theta)\nabla_\theta\large[\log q(\w\mid\theta)\large] f(\w, \theta)dw + \mathbb{E}_{q(\w\mid\theta)}[\nabla_\theta[f(w, \theta)]]\\ &amp;= \mathbb{E}_{q(\w\mid\theta)}\bigg [\nabla_\theta [\log q(\w\mid\theta)] f(\w, \theta)\bigg ] + \mathbb{E}_{q(\w\mid\theta)}[\nabla_\theta[f(\w, \theta)]]. \end{align*}\] <p>The problem is that the distribution in the first term is coupled with both $w$ and $\theta$. The value of the first term also depends on $\theta$ but $\theta$ is what we are tunning. We need to detach $\theta$ from the distribution. Then, we can apply the MC gradient estimator.</p> <p><strong>Why $q(\epsilon)d\epsilon = q(w\mid\theta)dw$?</strong></p> <p>For deterministic mapping $w = t(\epsilon, \theta)$, $q(\epsilon)d\epsilon = q(w\mid\theta)dw$ holds.</p> \[\begin{align*} q(\w\mid\theta)\frac{d\w}{d\epsilon} &amp;= q(\epsilon) \\ q(\w\mid\theta) &amp;= Kq(\epsilon)\\ G(\epsilon) &amp;= Kq(\epsilon)\\ \end{align*}\] <p><strong>Leibniz integral rule</strong></p> <blockquote> <p>Leibniz integral rule: $\displaystyle {\frac {d}{dx}}\left(\int _{a(x)}^{b(x)}f(x,t)\,dt\right)=f{\big (}x,b(x){\big )}\cdot {\frac {d}{dx}}b(x)-f{\big (}x,a(x){\big )}\cdot {\frac {d}{dx}}a(x)+\int _{a(x)}^{b(x)}{\frac {\partial }{\partial x}}f(x,t)\,dt$</p> </blockquote> <h1 id="mini-batches">Mini-batches</h1> <p>For each epoch of optimization, the training set is equally and randomly split into $M$ batches $\mathcal{D}_1,…,\mathcal{D}_M$. The loss can be rewritten as</p> \[\mathcal{F_i(\mathcal{D},\theta)}_ = \frac{1}{M}KL[q(\w\mid\theta)\mid P(w)] - \mathbb{E}_{q(\w\mid\theta)}[\log P(\mathcal{D}\mid\w)]\] <p>It can be used together with <a href="/blog/2022/LRT/">‘local parameterization technique’</a>.</p>]]></content><author><name></name></author><category term="Bayes"/><category term="Deep-Learning"/><summary type="html"><![CDATA[a brief review of paper 'Weight uncertainty in neural network']]></summary></entry></feed>