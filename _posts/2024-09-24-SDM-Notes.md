---
layout: post
title: Notes of Score Diffusion Models (updating) 
date: 2024-09-23 20:00:00-0400
description: basics of score diffusion models
comments: true
tags: Bayes Deep-Learning Diffusion-Model
---
**Table of Contents**
- [Unified Framework by Stochastic Differential Equations (SDE)](#unified-framework-by-stochastic-differential-equations-sde)
- [Background](#background)
  - [Data Perturbation with Itô SDE](#data-perturbation-with-itô-sde)
  - [Reverse SDE](#reverse-sde)
  - [Probability Flow ODE](#probability-flow-ode)


# Unified Framework by Stochastic Differential Equations (SDE)

In [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456), the authors propose a unified framework that connects the score-based model NCSN and DDPM through the perspective of Stochastic Differential Equations (SDE). They interpret the forward process (adding noise) and the backward process (denoising sampling) as SDE and reverse SDE, respectively.

<!-- ## Recap of NCSN and DDPM

DDPM and NCSN are essentially the same model but with different perturbation kernel. Let's use the notation from NCSN. In DDPM, we have a sequence of noise scales $0 < \beta_1 < \beta_2,...,\beta_N < 1$ and the perturbation kernel is $p_{\alpha_i}(\tilde{\x}\mid \x) = \N(\tilde{\x}\mid \sqrt{\alpha_i}\x, (1-\alpha_i)\I)$ where $\alpha_i=\prod_{j=1}^i\beta_j$. The objective function can be rewritten as 

$$
L_{simple} = (1-\alpha_i)\mathbb{E}_{p_{\alpha_i}}\mathbb{E}_{p_{data}}\sbr{\left\lVert\bm{s}_\theta(\tilde{\x},\alpha_i) + \frac{\tilde{\x}-\sqrt{\alpha_i}\x}{(1-\alpha_i)}\right\rVert^2_2}.
$$

We found that the scale factor $1-\alpha_i \propto \frac{1}{\left\lVert\frac{\tilde{\x}-\sqrt{\alpha_i}\x}{(1-\alpha_i)}\right\rVert^2_2}$, which is similar to the one in NCSN.  -->

# Background

## Data Perturbation with Itô SDE
The diffusion process $\{\x_t\}_{t=0}^T$ from the original input $\x_0$ to Gaussian noise $\x_T$ with continuous time variable $t\in\sbr{0,T}$ can be modeled by the Itô SDE
$$
\begin{equation}
d\x = \f(\x, t) dt + g(t)d\w_t
\end{equation}
$$

- $\w_t$ is the standard Wiener process.
- $\f(\x,t): \mathbb{R}^d \rightarrow \mathbb{R}^d$ are the drift coefficients and $d$ is the dimension of $\x$. It is always affine, resulting in $f(\x, t) = f(t)\x$ where $f(\cdot):\mathbb{R} \rightarrow \mathbb{R}$.
- $g(t): \mathbb{R} \rightarrow \mathbb{R}$ are the diffusion coefficients at time $t$.

Then, Eq. (1) can be rewritten as
$$
\begin{equation}
d\x = f(t)\x dt + g(t)d\w_t
\end{equation}
$$

The perturbation kernel of this SDE have the form 

$$
\begin{equation}
p(\x_t \mid \x_0) = \N(\x_t \mid s_t\x_0, s_t^2\sigma_t^2\I)
\end{equation}
$$

where

$$
\begin{equation}
s_t = \exp\nbr{\int_0^t f(\xi)d\xi} \quad \text{and}\quad
\sigma_t^2 = \int_0^t\nbr{\frac{g(\xi)}{s(\xi)}}^2d\xi
\end{equation}
$$

Here, I'll use $s_t$ and $s(t)$, and $\sigma_t$ and $\sigma(t)$ interchangeably for simplicity. The corresponding marginal distribution is obtained by 

$$
\begin{align}
p(\x_t) &= \int p(\x_t \mid \x_0)p_{data}(\x_0)d\x_0\\
&= s_t^{-d}\int p_{data}(\x_0)\N(\x_ts_t^{-1} \mid \x_0, \sigma_t^2\I)d\x_0\\
&= s_t^{-d}\int p_{data}(\x_0)\N(\x_ts_t^{-1}-\x_0 \mid 0, \sigma_t^2\I)d\x_0\\
&= s_t^{-d} \sbr{p_{data}(\x_0) * \N\nbr{0, \sigma_t^2\I}}(\x_ts_t^{-1})\\
&= s_t^{-d} p(\x_ts_t^{-1};\sigma_t)
\end{align}
$$


## Reverse SDE

The reverse diffusion process from $\x_T$ to $\x_0$ can also be modeled by the Itô SDE

$$
\begin{equation}
d\x = \sbr{f(t)\x - g_t^2\nabla_{\x}\log p(\x_t)}dt + g(t)d\w_t
\end{equation}
$$

## Probability Flow ODE

According to the [Fokker-Plank equation](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation), there exists a ODE which shares the same marginal distribution $p(\x_t)$ as the SDE. The ODE is called the probability flow ODE and is given by

$$
\begin{equation}
d\x = \sbr{f(t)\x - \frac{1}{2}g_t^2\nabla_{\x}\log p(\x)}dt
\end{equation}
$$

If we further build the ODE according to $s_t$ and $\sigma_t$, we have

$$
\begin{equation}
f(t) = \dot{s}_ts_t^{-1} \quad \text{and} \quad g_t = s_t\sqrt{2\dot{\sigma}_t\sigma_t}.
\end{equation}
$$

The derivation can be found in EDM paper (Eq. 28 and 34). Then, Eq. (10) can be rewritten as

$$
\begin{align}
d\x &= \sbr{\dot{s}_ts_t^{-1}\x - s_t^2\dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x)}dt\\
&= \sbr{\dot{s}_ts_t^{-1}\x - s_t^2\dot{\sigma}_t\sigma_t\nabla_{\x}\log p(\x s_t^{-1};\sigma_t)}dt
\end{align}
$$

where the last step is due to the marginal $p(\x_t) = s^{-1}_t p(\x_ts_t^{-1};\sigma_t)$ where $p(\x_ts_t^{-1};\sigma_t) = \sbr{p_{data}(\x_0) * \N\nbr{0, \sigma_t^2\I}}(\x_ts_t^{-1})$.

<!-- # Sampling Methods -->
