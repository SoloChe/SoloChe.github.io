<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yiming Che (车一鸣) | Revisit of Local Reparameterization Trick</title>
    <meta name="author" content="Yiming  Che" />
    <meta name="description" content="a brief review of paper Local Reparameterization Trick" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> 
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
    <!-- adde new font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Nunito+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;1,100&display=swap" rel="stylesheet">
    
    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/che.jpg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://soloche.github.io/blog/2022/LRT/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://SoloChe.github.io/">Yiming Che (车一鸣)</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">research</a>
              </li>

              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Revisit of Local Reparameterization Trick</h1>
    <p class="post-meta">December 26, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/tag/Bayes">
          <i class="fas fa-hashtag fa-sm"></i> Bayes</a>  
          <a href="/blog/tag/Deep-Learning">
          <i class="fas fa-hashtag fa-sm"></i> Deep-Learning</a>  
          

    </p>
  </header>

  <article class="post-content">
    <h1 id="brief-introduction">Brief introduction</h1>
<p>Paper can be found <a href="https://arxiv.org/pdf/1506.02557.pdf" target="_blank" rel="noopener noreferrer">here</a> [1].
This paper proposed <strong>local reparameterization technique</strong> to reduce the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability.</p>

<p>Notation: data $\mathcal{D} = \{\nbr{\x_i, y_i}\}_{i=1}^N$ , a single data $\mathcal{D^i} = \{(\x_i, y_i)\}$</p>
<h5 id="todo">Todo:</h5>
<ul class="task-list">
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Introduction</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Local Reparameterization Trick</li>
  <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>Variance of Gradient</li>
</ul>

<h2 id="variance-of-the-sgvb-estimator">Variance of the SGVB estimator</h2>
<p>Recall Bayesian framework $P(\w|\mathcal{D}) = \frac{P(\mathcal{D}\mid\w) P(\w)}{P(\mathcal{D})}$ and variational posterior $q(\w\mid\mathcal{D})$.
Recall the loss function negative ELBO:</p>

\[\begin{align}
L(\theta;\mathcal{D}) &amp;= \underbrace{KL[q(\w\mid\theta)\mid P(\w)]}_{\text{complexity cost}} - \underbrace{\mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]}_{\text{likelihood cost}}\\
\end{align}\]

<p>We assume that the complexity cost can be calculated analytically and the likelihood requires approximations. Hence, the variance is from the likelihood term which can be written as</p>

\[\begin{align}
\mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)] &amp;= \mathbb{E}_{q(\w\mid\mathcal{D})}\sbr{\sum_{i=1}^N\log P(\mathcal{D^i\mid\w})}\\
&amp;\approx\sum_{i=1}^{N}\sbr{\frac{1}{n}\sum_{j=1}^n\log P(\mathcal{D^i\mid\w^j})}\\
&amp;\approx\sum_{i=1}^{N}\log P(\mathcal{D^i\mid\w}) \quad \text{for $n=1$}
\end{align}\]

<p>where we use $n$ samples for each data $\mathcal{D}^i$ and there will be $N\times n$ samples of $\w$. If $n = 1$, Eq. (4) is the summation of log density of likelihood.</p>

<p>To construct the expected likelihood of the full data based on minibatches with size $M$, Eq. (3) is further written as</p>

\[L_{M} = \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]\approx \frac{N}{M}\sum_{i=1}^{M}\log P(\mathcal{D^i\mid\w}),\]

<p>where we set $n = 1$. In VAE paper [2], it says that if the batch size $M$ is large enough, $n$ could be $1$. The variance of $L_M$ can be decomposed as</p>

\[\begin{align}
Var[L_M] &amp;= \frac{N^2}{M^2}\nbr{\sum_{i=1}^{M}Var[L_i] + 2\sum_{i=1}^{M}\sum_{j=i+1}^{M}Cov[L_i,L_j]}\nonumber \\
&amp;= N^2\nbr{\frac{1}{M}Var[L_i] + \frac{M-1}{M}Cov[L_i, L_j]},
\end{align}\]

<p>where $L_M = \frac{N}{M}\sum_{i=1}^{M}L_i$ and $L_i = \log P(\mathcal{D^i\mid\w})$. The problem here is the coefficient $\frac{M-1}{M}$ ahead of covariance. It is not inversely proportional to $M$ as the variance term. Our target now is to remove the covariance, i.e., $Cov[L_i, L_j] = 0.$</p>

<h2 id="local-reparameterization-trick">Local reparameterization trick</h2>
<p>Some notations:</p>
<ul>
  <li>$\bm{A}: M \times 1000$ feature matrix with $M$ samples</li>
  <li>$\bm{W}: 1000 \times 1000$ weight matrix with the element $w_{i,j} \sim \N(\mu_{i,j}, \sigma^{2}_{i,j})$</li>
  <li>$\bm{B} = \bm{AW}$: $M \times 1000$ output before activation</li>
</ul>

<p>With the reparameterization trick, $w_{i,j} = \mu_{i,j} + \sigma_{i,j}\epsilon_{i,j}$ where $\epsilon_{i,j} \sim \N(0,1)$. To make $Cov[L_i, L_j] = 0$, we can sample $\w^1,…,\w^M$ for each observation in the minbatch because $\w^1,…,\w^M$ are i.i.d. and the randomness of the likelihood is only from the weight $\w$. However, it requires $M \times 1000 \times 1000$ samples for the minibatch only in a single layer.</p>

<p>To address the issue, we can sample $\bm{B}$ directly. $\bm{B=AW}$ where $\w \sim \N(\bm{\mu}, \bm{\Sigma})$ can be regarded as a sequence of linear combination of Gaussian random variables which is still Gaussian:</p>

\[\bm{B} \sim \N(\bs{A\mu},\bs{A\Sigma A^T})\]

<p>For each element $b_{m,j}$:</p>

\[\begin{align}
\gamma_{m,j} &amp;= \sum_{i=1}^{1000}a_{m,i}\mu_{i,j}\\
\delta_{m,j} &amp;= \sum_{i=1}^{1000}a^2_{m,i}\sigma^2_{i,j}
\end{align}\]

<p>To sample from $b_{m,j} = \gamma_{m,j} + \sqrt{\delta_{m,j}}\zeta_{m,j}$ where $\zeta_{m,j} \sim \N(0,1)$. In this case, the number of samples is $M \times 1000$.</p>

<h2 id="variance-of-gradient">Variance of gradient</h2>
<p>The local reparameterization trick also leads to lower variance of gradient compared to the method that sample $\w$ for each data. The gradient of posterior variance $\sigma^2_{i,j}$ with and without local reparameterization trick in Eq. (8) and (9):</p>

\[\begin{align}
\frac{\partial L_M}{\partial \sigma^2_{i,j}} &amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\partial b_{m,j}}{\partial \delta_{m,j}}\frac{\partial \delta_{m,j}}{\partial \sigma^2_{i,j}}\nonumber \\  
&amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\zeta_{m,j}}{2\sqrt{\delta_{m,j}}}a^2_{m,i}\\ 
\end{align}\]

\[\begin{equation}
\frac{\partial L_M}{\partial \sigma^2_{i,j}} = \frac{\partial L_M}{\partial b_{m,j}}\frac{\epsilon_{i,j}}{2\sigma_{i,j}}a_{m,i}
\end{equation}\]

<p>The two sources of randomness are $b_{m,j}$ and $\zeta_{m,j}$ for Eq. (8), $b_{m,j}$ and $\epsilon_{i,j}$ for Eq. (9). We may notice the difference of the subscript between random variable $\epsilon$ and $\zeta$. One intuitive explanation that Eq. (8) has lower variance than Eq. (9) is that there is more than one random variables $\epsilon$ contributes to $b_{m,j}$ in Eq. (9), i.e., $\epsilon_{i,j}$ for $i = 1,…,1000$ in this case. In contrast, only one variable $\zeta_{m,j}$ contributes to $b_{m,j}$ in Eq. (9). The mathematical analysis can be found in Appendix D from [1]. It decomposes the variance by total variance law and conditioning on $b_{m,j}$.</p>

<h1 id="references">References</h1>
<p>[1] Kingma, Durk P., Tim Salimans, and Max Welling. “Variational dropout and the local reparameterization trick.” Advances in neural information processing systems 28 (2015).</p>

<p>[2] Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013).</p>


  </article><div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'al-folio';
      var disqus_identifier = '/blog/2022/LRT';
      var disqus_title      = "Revisit of Local Reparameterization Trick";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by Disqus.</a>
</noscript>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Yiming  Che. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.
Last updated: June 15, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      loader: {load: ['[tex]/configmacros']},
      tex: {tags: 'ams', 
            packages: {'[+]': ['configmacros']},
            macros: {
                      R: '{\\mathbb{R}}',
                      f: '{\\bf f}',
                      w: '{\\boldsymbol{w}}',
                      x: '{\\boldsymbol{x}}',
                      I: '{\\boldsymbol{I}}',
                      nbr: ['\\left(#1\\right)',1],
                      sbr: ['\\left[#1\\right]',1],
                      bm: ['\\boldsymbol{#1}',1, ],
                      bs: ['\\boldsymbol{#1}',1, ],
                      N: '{\\mathcal{N}}',
                      argmax:['\\underset{#1}{\\operatorname{argmax}}', 1],
                      argmin:['\\underset{#1}{\\operatorname{argmin}}', 1]
                    },
            inlineMath: [['$', '$'], ["\\(", "\\)"]],
            displayMath: [['$$', '$$'], ['\\[', '\\]']]
          },
      
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

