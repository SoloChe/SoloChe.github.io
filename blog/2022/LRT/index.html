<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Revisit of Local Reparameterization Trick | Yiming Che (车一鸣) </title> <meta name="author" content="Yiming Che"> <meta name="description" content="a brief review of paper Local Reparameterization Trick"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/che.jpg?cb45d92e8d9c4d4cdb80894d70e4a1ef"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://soloche.github.io/blog/2022/LRT/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yiming Che (车一鸣) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Revisit of Local Reparameterization Trick</h1> <p class="post-meta"> Created in December 26, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/bayes"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayes</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep-Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="brief-introduction">Brief introduction</h1> <p>Paper can be found <a href="https://arxiv.org/pdf/1506.02557.pdf" rel="external nofollow noopener" target="_blank">here</a> [1]. This paper proposed <strong>local reparameterization technique</strong> to reduce the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability.</p> <p>Notation: data $\mathcal{D} = \{\nbr{\x_i, y_i}\}_{i=1}^N$ , a single data $\mathcal{D^i} = \{(\x_i, y_i)\}$</p> <h5 id="todo">Todo:</h5> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Introduction</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Local Reparameterization Trick</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked>Variance of Gradient</li> </ul> <h2 id="variance-of-the-sgvb-estimator">Variance of the SGVB estimator</h2> <p>Recall Bayesian framework $P(\w|\mathcal{D}) = \frac{P(\mathcal{D}\mid\w) P(\w)}{P(\mathcal{D})}$ and variational posterior $q(\w\mid\mathcal{D})$. Recall the loss function negative ELBO:</p> \[\begin{align} L(\theta;\mathcal{D}) &amp;= \underbrace{KL[q(\w\mid\theta)\mid P(\w)]}_{\text{complexity cost}} - \underbrace{\mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]}_{\text{likelihood cost}}\\ \end{align}\] <p>We assume that the complexity cost can be calculated analytically and the likelihood requires approximations. Hence, the variance is from the likelihood term which can be written as</p> \[\begin{align} \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)] &amp;= \mathbb{E}_{q(\w\mid\mathcal{D})}\sbr{\sum_{i=1}^N\log P(\mathcal{D^i\mid\w})}\\ &amp;\approx\sum_{i=1}^{N}\sbr{\frac{1}{n}\sum_{j=1}^n\log P(\mathcal{D^i\mid\w^j})}\\ &amp;\approx\sum_{i=1}^{N}\log P(\mathcal{D^i\mid\w}) \quad \text{for $n=1$} \end{align}\] <p>where we use $n$ samples for each data $\mathcal{D}^i$ and there will be $N\times n$ samples of $\w$. If $n = 1$, Eq. (4) is the summation of log density of likelihood.</p> <p>To construct the expected likelihood of the full data based on minibatches with size $M$, Eq. (3) is further written as</p> \[L_{M} = \mathbb{E}_{q(\w\mid\mathcal{D})}[\log P(\mathcal{D}\mid\w)]\approx \frac{N}{M}\sum_{i=1}^{M}\log P(\mathcal{D^i\mid\w}),\] <p>where we set $n = 1$. In VAE paper [2], it says that if the batch size $M$ is large enough, $n$ could be $1$. The variance of $L_M$ can be decomposed as</p> \[\begin{align} Var[L_M] &amp;= \frac{N^2}{M^2}\nbr{\sum_{i=1}^{M}Var[L_i] + 2\sum_{i=1}^{M}\sum_{j=i+1}^{M}Cov[L_i,L_j]}\nonumber \\ &amp;= N^2\nbr{\frac{1}{M}Var[L_i] + \frac{M-1}{M}Cov[L_i, L_j]}, \end{align}\] <p>where $L_M = \frac{N}{M}\sum_{i=1}^{M}L_i$ and $L_i = \log P(\mathcal{D^i\mid\w})$. The problem here is the coefficient $\frac{M-1}{M}$ ahead of covariance. It is not inversely proportional to $M$ as the variance term. Our target now is to remove the covariance, i.e., $Cov[L_i, L_j] = 0.$</p> <h2 id="local-reparameterization-trick">Local reparameterization trick</h2> <p>Some notations:</p> <ul> <li>$\bm{A}: M \times 1000$ feature matrix with $M$ samples</li> <li>$\bm{W}: 1000 \times 1000$ weight matrix with the element $w_{i,j} \sim \N(\mu_{i,j}, \sigma^{2}_{i,j})$</li> <li>$\bm{B} = \bm{AW}$: $M \times 1000$ output before activation</li> </ul> <p>With the reparameterization trick, $w_{i,j} = \mu_{i,j} + \sigma_{i,j}\epsilon_{i,j}$ where $\epsilon_{i,j} \sim \N(0,1)$. To make $Cov[L_i, L_j] = 0$, we can sample $\w^1,…,\w^M$ for each observation in the minbatch because $\w^1,…,\w^M$ are i.i.d. and the randomness of the likelihood is only from the weight $\w$. However, it requires $M \times 1000 \times 1000$ samples for the minibatch only in a single layer.</p> <p>To address the issue, we can sample $\bm{B}$ directly. $\bm{B=AW}$ where $\w \sim \N(\bm{\mu}, \bm{\Sigma})$ can be regarded as a sequence of linear combination of Gaussian random variables which is still Gaussian:</p> \[\bm{B} \sim \N(\bs{A\mu},\bs{A\Sigma A^T})\] <p>For each element $b_{m,j}$:</p> \[\begin{align} \gamma_{m,j} &amp;= \sum_{i=1}^{1000}a_{m,i}\mu_{i,j}\\ \delta_{m,j} &amp;= \sum_{i=1}^{1000}a^2_{m,i}\sigma^2_{i,j} \end{align}\] <p>To sample from $b_{m,j} = \gamma_{m,j} + \sqrt{\delta_{m,j}}\zeta_{m,j}$ where $\zeta_{m,j} \sim \N(0,1)$. In this case, the number of samples is $M \times 1000$.</p> <h2 id="variance-of-gradient">Variance of gradient</h2> <p>The local reparameterization trick also leads to lower variance of gradient compared to the method that sample $\w$ for each data. The gradient of posterior variance $\sigma^2_{i,j}$ with and without local reparameterization trick in Eq. (8) and (9):</p> \[\begin{align} \frac{\partial L_M}{\partial \sigma^2_{i,j}} &amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\partial b_{m,j}}{\partial \delta_{m,j}}\frac{\partial \delta_{m,j}}{\partial \sigma^2_{i,j}}\nonumber \\ &amp;= \frac{\partial L_M}{\partial b_{m,j}}\frac{\zeta_{m,j}}{2\sqrt{\delta_{m,j}}}a^2_{m,i}\\ \end{align}\] \[\begin{equation} \frac{\partial L_M}{\partial \sigma^2_{i,j}} = \frac{\partial L_M}{\partial b_{m,j}}\frac{\epsilon_{i,j}}{2\sigma_{i,j}}a_{m,i} \end{equation}\] <p>The two sources of randomness are $b_{m,j}$ and $\zeta_{m,j}$ for Eq. (8), $b_{m,j}$ and $\epsilon_{i,j}$ for Eq. (9). We may notice the difference of the subscript between random variable $\epsilon$ and $\zeta$. One intuitive explanation that Eq. (8) has lower variance than Eq. (9) is that there is more than one random variables $\epsilon$ contributes to $b_{m,j}$ in Eq. (9), i.e., $\epsilon_{i,j}$ for $i = 1,…,1000$ in this case. In contrast, only one variable $\zeta_{m,j}$ contributes to $b_{m,j}$ in Eq. (9). The mathematical analysis can be found in Appendix D from [1]. It decomposes the variance by total variance law and conditioning on $b_{m,j}$.</p> <h1 id="references">References</h1> <p>[1] Kingma, Durk P., Tim Salimans, and Max Welling. “Variational dropout and the local reparameterization trick.” Advances in neural information processing systems 28 (2015).</p> <p>[2] Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013).</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yiming Che. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 08, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",packages:{"[+]":["configmacros"]},macros:{R:"{\\mathbb{R}}",f:"{\\bf f}",w:"{\\boldsymbol{w}}",x:"{\\boldsymbol{x}}",I:"{\\boldsymbol{I}}",nbr:["\\left(#1\\right)",1],sbr:["\\left[#1\\right]",1],bm:["\\boldsymbol{#1}",1],bs:["\\boldsymbol{#1}",1],N:"{\\mathcal{N}}",argmax:["\\underset{#1}{\\operatorname{argmax}}",1],argmin:["\\underset{#1}{\\operatorname{argmin}}",1]},inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>